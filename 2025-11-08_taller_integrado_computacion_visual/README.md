
# Taller Integral de Computaci√≥n Visual

## 5. Visualizaci√≥n de im√°genes y video 360¬∞

### üéØ Concepto

Un **visor inmersivo** que permite explorar im√°genes o videos 360¬∞ dentro de una esfera virtual usando **Three.js** y **React Three Fiber**, simulando una experiencia de realidad virtual b√°sica en el navegador.

---

### ‚öôÔ∏è Funcionalidades principales

- Renderiza una **imagen HDRI panor√°mica** o un **video 360¬∞** como fondo.  
- Control de c√°mara con **OrbitControls** (rotaci√≥n libre).  
- Botones para cambiar entre modo imagen y modo video.  
- Video 360¬∞ proyectado internamente sobre una esfera invertida (`BackSide`).

---

### üß∞ Dependencias e instalaci√≥n

```bash
npm install three @react-three/fiber @react-three/drei
```

---

### ‚ñ∂Ô∏è Ejecuci√≥n

En un proyecto React con Vite o Create React App:

```bash
npm run dev
```

Coloca tus archivos multimedia en `/public`:
- `/bloem_field_sunrise_4k.hdr`  
- `/20257855-hd_1920_1080_60fps.mp4`

---

### üß† Fragmento clave

```jsx
<mesh ref={meshRef} scale={[-1, 1, 1]}>
  <sphereGeometry args={[500, 60, 40]} />
  <meshBasicMaterial side={THREE.BackSide} />
</mesh>
```

---

### üì∏ Evidencia gr√°fica

![Video actividad 5](/2025-11-08_taller_integrado_computacion_visual/media/actividad-5/actividad_5.gif)

---

### üí° Reflexi√≥n

**Aprendizajes:** manejo de texturas HDR y videos como `VideoTexture`, control de c√°mara con OrbitControls.

**Retos t√©cnicos:** sincronizaci√≥n de texturas de video y rendimiento en navegadores.

**Mejoras posibles:** agregar puntos interactivos (hotspots), audio espacial y soporte para visores VR/WebXR.

---


## 7. Gestos con c√°mara web (MediaPipe Hands)

### üéØ Concepto

Un **experimento visual interactivo** basado en visi√≥n por computadora, donde el usuario puede **dibujar en el aire** usando los gestos de su mano detectados por la c√°mara. Cada gesto realiza una acci√≥n sobre el lienzo digital.

---

### ‚öôÔ∏è Funcionalidades principales

- Detecci√≥n en tiempo real de la mano con **MediaPipe Hands**.  
- Clasificaci√≥n de gestos: ‚úã `OPEN`, üëä `FIST`, ‚òùÔ∏è `POINT`, ‚úåÔ∏è `VICTORY`, üëç `THUMB_UP`, ü§è `PINCH`, üëå `OK`.  
- **Mapeo visual interactivo**:
  - `POINT`: dibujar con el √≠ndice.  
  - `PINCH`: cambiar color.  
  - `FIST`: limpiar pantalla.  
  - `OPEN`: pausar/reanudar.  
  - `THUMB_UP`: aumentar grosor del pincel.  
  - `VICTORY`: disminuir grosor.  
  - `OK`: guardar snapshot.

---

### üß∞ Dependencias e instalaci√≥n

```bash
pip install opencv-python mediapipe numpy
```

---

### ‚ñ∂Ô∏è Ejecuci√≥n

```bash
python gestos_con_camara_web.py
```

- Presiona `ESC` para salir.  
- Presiona `C` para limpiar el lienzo manualmente.  

---

### üß† Fragmento clave

```python
if gesture == 'POINT' and mode_paint:
    pts_deque.append(idx_tip)
elif gesture == 'PINCH':
    color_idx = (color_idx + 1) % len(colors)
elif gesture == 'FIST':
    canvas[:] = 0
```

---

### üì∏ Evidencia gr√°fica

![Video actividad 7](/2025-11-08_taller_integrado_computacion_visual/media/actividad-7/actividad_7.gif)

---

### üí° Reflexi√≥n

Este experimento permiti√≥ comprender la **traducci√≥n de se√±ales corporales a acciones digitales**.  
**Aprendizajes:** procesamiento de landmarks, suavizado temporal, y calibraci√≥n de umbrales de detecci√≥n.  
**Retos t√©cnicos:** la variabilidad de iluminaci√≥n y la velocidad de procesamiento en tiempo real.  
**Posibles mejoras:** detecci√≥n de m√∫ltiples manos y uso de modelos de aprendizaje profundo para reconocimiento din√°mico de gestos.

---

## 8. Reconocimiento de voz y control por comandos

### üß∞ Dependencias e instalaci√≥n
```bash
pip install SpeechRecognition pyaudio pyttsx3 pygame numpy
```
---
### ‚ñ∂Ô∏è Ejecuci√≥n
```bash
python voice_control.py
```
Aseg√∫rate de tener un micr√≥fono conectado y configurado correctamente.
---
### üß† Fragmento clave
```python
def asr_worker():
    while True:
        audio = audio_q.get()
        try:
            # Online (simple y robusto). Si requieres offline, cambia a recognize_sphinx(language="es-ES")
            text = r.recognize_google(audio, language=LANG).lower().strip()
            print("Heard:", text)
            executed = False
            for key, (op, val) in COMMANDS.items():
                if key in text:
                    state.apply(op, val)
                    say(key)
                    executed = True
            if not executed:
                say("No entendido")
        except Exception as e:
            print("ASR error:", e)
```
---
### üí° Reflexi√≥n
La implementaci√≥n de reconocimiento de voz permite una interacci√≥n m√°s natural y fluida con el sistema.  
**Aprendizajes:** integraci√≥n de bibliotecas de reconocimiento de voz, manejo de excepciones y control visual mediante comandos de voz.  
**Retos t√©cnicos:** variabilidad en la calidad del audio y la precisi√≥n del reconocimiento.  
**Mejoras posibles:** agregar soporte para m√∫ltiples idiomas y comandos personalizados.

## 9. Interfaces multimodales (voz + gestos)

### üéØ Concepto

Extiende el experimento anterior integrando **comandos de voz (Vosk)** y **gestos simult√°neos** para crear una **interfaz multimodal** donde ambos canales (visual y auditivo) se fusionan para controlar el lienzo.

---

### ‚öôÔ∏è Funcionalidades principales

- Reconocimiento de voz **offline** con Vosk.  
- Detecci√≥n de gestos con **MediaPipe Hands**.  
- **Fusi√≥n temporal** de eventos (`voz + gesto`) para ejecutar acciones combinadas.  
- **Canvas interactivo** que responde a comandos:
  - ‚ÄúColor rojo‚Äù + gesto `PINCH` ‚Üí cambia color.  
  - ‚ÄúGuardar‚Äù + gesto `OK` ‚Üí guarda snapshot.  
  - ‚ÄúBorrar‚Äù + `FIST` ‚Üí limpia lienzo.  
  - `THUMB_UP` / `VICTORY` ‚Üí aumenta o disminuye el pincel.  

---

### üß∞ Dependencias e instalaci√≥n

```bash
pip install opencv-python mediapipe numpy vosk sounddevice
```

> ‚ö†Ô∏è Descarga el modelo de voz Vosk en espa√±ol:
```bash
mkdir -p models
wget https://alphacephei.com/vosk/models/vosk-model-small-es-0.42.zip
unzip vosk-model-small-es-0.42.zip -d models/
```

---

### ‚ñ∂Ô∏è Ejecuci√≥n

```bash
python main.py
```

Requiere c√°mara y micr√≥fono activos.

---

### üß† Fragmento clave

```python
if 'color' in text:
    pinch = next((e for e in recent if e['name']=='PINCH'), None)
    if pinch:
        execute_action({'action':'set_color','color_name':chosen, 'source':'voice+pinch'})
```

---

### üì∏ Evidencia gr√°fica
![Video actividad 9](/2025-11-08_taller_integrado_computacion_visual/media/actividad-9/actividad_9.gif)

---

### üí° Reflexi√≥n

Combinar voz y gestos introduce **sinergia cognitiva** en la interacci√≥n hombre-m√°quina.  
**Aprendizajes:** uso de hilos para reconocimiento en paralelo, sincronizaci√≥n de eventos y arquitectura multimodal.  
**Retos t√©cnicos:** latencia en la sincronizaci√≥n voz-gesto y manejo concurrente del micr√≥fono y la c√°mara.  
**Mejoras futuras:** integrar un m√≥dulo de contexto para aprender patrones de interacci√≥n del usuario o comandos personalizados.

## 10. Simulaci√≥n BCI (EEG sint√©tico y control)

### üß∞ Dependencias e instalaci√≥n
```bash
pip install -r requirements.txt
```
---
### ‚ñ∂Ô∏è Ejecuci√≥n
```bash
python eeg_sim.py
```
Aseg√∫rate de tener los permisos necesarios para acceder a los dispositivos de entrada si es necesario.
---
### üß† Fragmento clave
```python
import numpy as np
import matplotlib.pyplot as plt

# Simulaci√≥n de se√±ales EEG sint√©ticas
def generate_eeg_signal(duration=10, fs=256):
  t = np.linspace(0, duration, duration * fs)
  signal = np.sin(2 * np.pi * 10 * t) + np.random.normal(0, 0.5, t.shape)
  return t, signal

# Visualizaci√≥n de la se√±al
t, eeg_signal = generate_eeg_signal()
plt.plot(t, eeg_signal)
plt.title('Se√±al EEG Sint√©tica')
plt.xlabel('Tiempo (s)')
plt.ylabel('Amplitud')
plt.show()
```
---
### üí° Reflexi√≥n
La simulaci√≥n de EEG permite explorar patrones de actividad cerebral y su relaci√≥n con el control de dispositivos.  
**Aprendizajes:** generaci√≥n de se√±ales sint√©ticas y visualizaci√≥n de datos en tiempo real.  
**Retos t√©cnicos:** modelar adecuadamente la variabilidad de las se√±ales EEG reales.  
**Mejoras posibles:** integrar datos reales de EEG y aplicar t√©cnicas de procesamiento de se√±ales para an√°lisis m√°s profundos.

## 10. Simulaci√≥n BCI (EEG sint√©tico y control)

### üéØ Concepto

Simulaci√≥n de se√±ales EEG sint√©ticas que permiten explorar patrones de actividad cerebral y su relaci√≥n con el control visual.

---

### üß∞ Dependencias e instalaci√≥n

```bash
pip install numpy scipy pygame
```

---

### ‚ñ∂Ô∏è Ejecuci√≥n

```bash
python eeg_sim.py
```

Aseg√∫rate de tener los permisos necesarios para acceder a los dispositivos de entrada si es necesario.

---

### üß† Fragmento clave

```python
# -*- coding: utf-8 -*-
import numpy as np
from scipy.signal import butter, lfilter, welch
import pygame, random

# -------- Config EEG --------
FS = 256                   # Hz
WIN = 2.5                  # s por ventana
N  = int(FS*WIN)
ALPHA = (8,12)
BETA  = (13,30)
TH_ALPHA = 2.2             # umbral relativo simple
TH_BETA  = 2.0

# -------- S√≠ntesis ----------
def synth_eeg(n, fs, a_amp=1.0, b_amp=0.8, noise=0.4):
  t = np.arange(n)/fs
  alpha = a_amp*np.sin(2*np.pi*10*t + np.random.rand()*2*np.pi)
  beta  = b_amp*np.sin(2*np.pi*20*t + np.random.rand()*2*np.pi)
  pink  = noise*np.cumsum(np.random.randn(n)); pink /= np.max(np.abs(pink)+1e-6)
  return alpha + beta + 0.4*pink

# ... (resto del c√≥digo)
```

---

### üí° Reflexi√≥n

La simulaci√≥n de EEG permite explorar patrones de actividad cerebral y su relaci√≥n con el control de dispositivos.  
**Aprendizajes:** generaci√≥n de se√±ales sint√©ticas y visualizaci√≥n de datos en tiempo real.  
**Retos t√©cnicos:** modelar adecuadamente la variabilidad de las se√±ales EEG reales.  
**Mejoras posibles:** integrar datos reales de EEG y aplicar t√©cnicas de procesamiento de se√±ales para an√°lisis m√°s profundos.

## 11. Espacios proyectivos y matrices de proyecci√≥n
### üéØ Concepto
Simulaci√≥n de proyecciones en 3D utilizando c√°maras perspectiva y ortogr√°fica para visualizar la diferencia entre ambas.

---
### ‚öôÔ∏è Funcionalidades principales
- Alternar entre c√°mara perspectiva y ortogr√°fica con la tecla `[C]`.  
- Activar/desactivar el mapa de profundidad con la tecla `[D]`.  
- Visualizaci√≥n de un objeto 3D (Torus Knot) en un entorno iluminado.

---
### ‚ñ∂Ô∏è Ejecuci√≥n
Abre el archivo HTML en un navegador compatible con WebGL.

---
### üß† Fragmento clave
```javascript
const persp = new THREE.PerspectiveCamera(60, innerWidth / innerHeight, 0.1, 100);
const ortho = new THREE.OrthographicCamera(-orthoH * innerWidth / innerHeight, orthoH * innerWidth / innerHeight, orthoH, -orthoH, 0.1, 100);
```
---
### üí° Reflexi√≥n
La comparaci√≥n entre proyecciones perspectiva y ortogr√°fica permite entender c√≥mo afectan la percepci√≥n de la profundidad y la escala en entornos 3D.  
**Aprendizajes:** manejo de diferentes tipos de c√°maras en Three.js y su impacto visual.  
**Retos t√©cnicos:** optimizaci√≥n del rendimiento al alternar entre c√°maras.  
**Mejoras posibles:** agregar m√°s geometr√≠as y efectos visuales para enriquecer la experiencia.
