# Subsistema 1: Detecci√≥n y Segmentaci√≥n Inteligente

## üìã √çndice
1. [Descripci√≥n General](#descripci√≥n-general)
2. [Caracter√≠sticas](#caracter√≠sticas)
3. [Tecnolog√≠as y Librer√≠as](#tecnolog√≠as-y-librer√≠as)
4. [Requisitos del Sistema](#requisitos-del-sistema)
5. [Instalaci√≥n](#instalaci√≥n)
6. [Estructura del Proyecto](#estructura-del-proyecto)
7. [Gu√≠a de Uso Detallada](#gu√≠a-de-uso-detallada)
---

## üéØ Descripci√≥n General

Sistema avanzado de visi√≥n por computador que combina:
- **YOLO** para detecci√≥n de objetos en tiempo real
- **MediaPipe** para seguimiento de manos, pose y rostro
- **CLIP** para embeddings visuales y b√∫squeda sem√°ntica
- **Dashboard interactivo** con m√©tricas en tiempo real

Este subsistema est√° dise√±ado para ser **completamente funcional e independiente**, con capacidad de procesamiento de webcam, im√°genes y videos.

---

## ‚ú® Caracter√≠sticas

### 1. **Detecci√≥n de Objetos con YOLO**
- Detecci√≥n en tiempo real con webcam (30+ FPS)
- Soporte para 80+ clases de COCO dataset
- Procesamiento de im√°genes y videos
- Exportaci√≥n de anotaciones en JSON
- Configuraci√≥n de umbrales de confianza e IoU

### 2. **Seguimiento Multi-Modal con MediaPipe**
- **Manos**: 21 puntos de referencia por mano, detecci√≥n de gestos
- **Pose**: 33 puntos del cuerpo completo
- **Rostro**: 468 puntos del mesh facial (opcional)
- Reconocimiento de gestos b√°sicos (pu√±o, se√±alar, palma abierta, paz)
- C√°lculo de m√©tricas de postura

### 3. **Embeddings y B√∫squeda con CLIP**
- Extracci√≥n de vectores de caracter√≠sticas visuales
- B√∫squeda de im√°genes por texto en lenguaje natural
- Visualizaci√≥n de espacios de embeddings (PCA y t-SNE)
- C√°lculo de similitud coseno entre im√°genes y texto

### 4. **Dashboard Interactivo**
- Visualizaci√≥n en tiempo real de detecciones
- Gr√°ficas de rendimiento (FPS, latencia)
- An√°lisis de distribuci√≥n de objetos
- Exportaci√≥n de datos en m√∫ltiples formatos

---

## üîß Tecnolog√≠as y Librer√≠as

### **Visi√≥n por Computador**
```
opencv-python >= 4.8.0          # Procesamiento de im√°genes y video
opencv-contrib-python >= 4.8.0  # M√≥dulos adicionales de OpenCV
ultralytics >= 8.0.0            # YOLO v8/v9 para detecci√≥n de objetos
mediapipe >= 0.10.0             # Seguimiento de manos, pose y rostro
```

### **Deep Learning**
```
torch >= 2.0.0                  # PyTorch para modelos de deep learning
torchvision >= 0.15.0           # Utilidades de visi√≥n para PyTorch
clip @ git+https://...          # CLIP de OpenAI para embeddings
pillow >= 10.0.0                # Manipulaci√≥n de im√°genes
scikit-image >= 0.21.0          # Procesamiento cient√≠fico de im√°genes
```

### **Ciencia de Datos y Visualizaci√≥n**
```
numpy >= 1.24.0                 # Operaciones num√©ricas
pandas >= 2.0.0                 # Manipulaci√≥n de datos tabulares
scikit-learn >= 1.3.0           # PCA, t-SNE y m√©tricas
matplotlib >= 3.7.0             # Visualizaci√≥n de gr√°ficas est√°ticas
seaborn >= 0.12.0               # Visualizaci√≥n estad√≠stica
plotly >= 5.14.0                # Gr√°ficas interactivas
```

### **Dashboard y Web**
```
streamlit >= 1.28.0             # Framework para aplicaciones web
streamlit-webrtc >= 0.45.0      # Soporte de webcam en Streamlit
fastapi >= 0.104.0              # API REST (opcional)
uvicorn >= 0.24.0               # Servidor ASGI
websockets >= 12.0              # Comunicaci√≥n en tiempo real
```

### **Utilidades**
```
tqdm >= 4.66.0                  # Barras de progreso
python-dotenv >= 1.0.0          # Variables de entorno
pyyaml >= 6.0                   # Configuraci√≥n en YAML
openpyxl >= 3.1.0               # Exportaci√≥n a Excel
```

---

## üíª Requisitos del Sistema

### **M√≠nimos**
- **Sistema Operativo**: Windows 10/11, Ubuntu 20.04+, macOS 10.15+
- **CPU**: Intel Core i5 o equivalente
- **RAM**: 8 GB
- **Python**: 3.8 o superior
- **Webcam**: Cualquier c√°mara compatible con OpenCV

### **Recomendados**
- **CPU**: Intel Core i7/i9 o AMD Ryzen 7/9
- **RAM**: 16 GB o m√°s
- **GPU**: NVIDIA con CUDA (GTX 1060 o superior) para mejor rendimiento
- **Espacio en disco**: 5 GB libres

### **Para GPU (Opcional pero Recomendado)**
```bash
# CUDA Toolkit 11.8 o superior
# cuDNN compatible
```

---

## üöÄ Instalaci√≥n

### **Paso 1: Clonar el Repositorio**
```bash
git clone [URL_DEL_REPOSITORIO]
cd Subsistema_1-Deteccion_y_segmentacion_inteligente
```

### **Paso 2: Crear Entorno Virtual**

**En Linux/macOS:**
```bash
python3.11 -m venv venv
source venv/bin/activate
```

**En Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

### **Paso 3: Instalar Dependencias**

**Opci√≥n A: Instalaci√≥n Completa**
```bash
pip install -r requirements.txt
```

**Opci√≥n B: Instalaci√≥n con GPU (NVIDIA CUDA)**
```bash
# Primero instala PyTorch con soporte CUDA
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# Luego el resto de dependencias
pip install -r requirements.txt
```

### **Paso 4: Descargar Modelos YOLO (Autom√°tico)**
Los modelos se descargan autom√°ticamente la primera vez que los usas:
- `yolov8n.pt` - Nano (m√°s r√°pido, menos preciso) ~6MB
- `yolov8s.pt` - Small (balance) ~22MB
- `yolov8m.pt` - Medium (m√°s preciso) ~52MB
- `yolov8l.pt` - Large (m√°s preciso, m√°s lento) ~83MB

### **Paso 5: Verificar Instalaci√≥n**
```bash
# Verificar que OpenCV funciona
python -c "import cv2; print('OpenCV:', cv2.__version__)"

# Verificar YOLO
python -c "from ultralytics import YOLO; print('YOLO: OK')"

# Verificar MediaPipe
python -c "import mediapipe as mp; print('MediaPipe: OK')"

# Verificar Streamlit
streamlit --version
```

### **Paso 6: Crear Estructura de Directorios**
```bash
mkdir -p data/input data/output results/images results/videos results/embeddings exports snapshots
```

---

## üìÅ Estructura del Proyecto

```
subsystem_1_detection/
‚îÇ
‚îú‚îÄ‚îÄ detectors/                      # M√≥dulos de detecci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ yolo_detector.py           # Detecci√≥n con YOLO
‚îÇ   ‚îú‚îÄ‚îÄ mediapipe_detector.py      # Seguimiento con MediaPipe
‚îÇ   ‚îî‚îÄ‚îÄ clip_embeddings.py         # Embeddings con CLIP
‚îÇ
‚îú‚îÄ‚îÄ dashboard/                      # Dashboard web
‚îÇ   ‚îî‚îÄ‚îÄ streamlit_app.py           # Aplicaci√≥n Streamlit
‚îÇ
‚îú‚îÄ‚îÄ data/                           # Datos de entrada
‚îÇ   ‚îú‚îÄ‚îÄ input/                     # Im√°genes/videos de entrada
‚îÇ   ‚îú‚îÄ‚îÄ output/                    # Resultados procesados
‚îÇ   ‚îî‚îÄ‚îÄ annotations/               # Anotaciones JSON
‚îÇ
‚îú‚îÄ‚îÄ results/                        # Resultados generados
‚îÇ   ‚îú‚îÄ‚îÄ images/                    # Im√°genes anotadas
‚îÇ   ‚îú‚îÄ‚îÄ videos/                    # Videos procesados
‚îÇ   ‚îú‚îÄ‚îÄ embeddings/                # Embeddings y visualizaciones
‚îÇ
‚îú‚îÄ‚îÄ exports/                        # Datos exportados
‚îÇ   ‚îú‚îÄ‚îÄ detections_*.json          # Exportaciones JSON
‚îÇ   ‚îú‚îÄ‚îÄ detections_*.csv           # Exportaciones CSV
‚îÇ   ‚îî‚îÄ‚îÄ detections_*.xlsx          # Exportaciones Excel
‚îÇ
‚îú‚îÄ‚îÄ snapshots/                      # Capturas de webcam
‚îÇ
‚îú‚îÄ‚îÄ demo.py                         # Script de demostraci√≥n
‚îú‚îÄ‚îÄ requirements.txt                # Dependencias
‚îú‚îÄ‚îÄ README.md                       # Este archivo
‚îî‚îÄ‚îÄ .gitignore
```

---

## üìñ Gu√≠a de Uso Detallada

### **A. Dashboard Web (Streamlit) - Uso Completo**

#### **1. Iniciar el Dashboard**
```bash
# Desde la ra√≠z del proyecto
streamlit run dashboard/streamlit_app.py
```

Esto abrir√° autom√°ticamente tu navegador en `http://localhost:8501`

#### **2. Interfaz del Dashboard**

##### **Barra Lateral (Configuraci√≥n)**

**a) Modo de Detecci√≥n:**
- **YOLO Object Detection**: Detecta 80+ tipos de objetos (personas, veh√≠culos, animales, etc.)
- **MediaPipe Tracking**: Sigue manos, cuerpo y rostro
- **Combined**: Usa ambos simult√°neamente

**b) Configuraci√≥n de Modelos:**

Para **YOLO**:
- **Model Size**: Selecciona el tama√±o del modelo
  - `yolov8n.pt` ‚Üí M√°s r√°pido (30+ FPS), menos preciso
  - `yolov8s.pt` ‚Üí Balance (25 FPS), buena precisi√≥n ‚≠ê **Recomendado**
  - `yolov8m.pt` ‚Üí M√°s lento (15 FPS), mejor precisi√≥n
  - `yolov8l.pt` ‚Üí M√°s lento (10 FPS), m√°xima precisi√≥n
  
- **Confidence Threshold** (0.0 - 1.0): 
  - Valor bajo (0.3): Detecta m√°s objetos, m√°s falsos positivos
  - Valor alto (0.7): Solo detecciones muy seguras, puede perder objetos
  - **Recomendado**: 0.5

- **IoU Threshold** (0.0 - 1.0):
  - Controla la eliminaci√≥n de cajas duplicadas
  - **Recomendado**: 0.45

Para **MediaPipe**:
- **Track Hands**: ‚úÖ Activa detecci√≥n de manos y gestos
- **Track Pose**: ‚úÖ Activa seguimiento de pose corporal
- **Track Face**: ‚òê Activa mesh facial (468 puntos, consume m√°s CPU)
- **Min Detection Confidence**: Umbral de confianza (recomendado: 0.5)

**c) Fuente de Entrada:**
- **Webcam**: Detecci√≥n en tiempo real
- **Upload Image**: Procesa una imagen
- **Upload Video**: Procesa un video completo

**d) Configuraci√≥n de Rendimiento:**
- **Device**: 
  - `cpu` ‚Üí Compatible con todos, m√°s lento
  - `cuda` ‚Üí Requiere GPU NVIDIA, 3-5x m√°s r√°pido

#### **3. Pesta√±a: üìπ Live Detection**

##### **Usando Webcam:**

**Paso a paso:**

1. **Selecciona el modo**: Ejemplo: "YOLO Object Detection"

2. **Configura par√°metros** en la barra lateral:
   ```
   Model Size: yolov8s.pt
   Confidence: 0.5
   IoU: 0.45
   Device: cpu
   ```

3. **Selecciona "Webcam"** en Input Source

4. **Configura Camera ID**: 
   - 0 = Webcam por defecto
   - 1 = Segunda c√°mara (si existe)

5. **Click en "‚ñ∂Ô∏è Start"**: 
   - Se inicializar√° el detector (tarda 2-5 segundos la primera vez)
   - Ver√°s el mensaje "‚úÖ YOLO detector initialized!"
   - Comenzar√° a mostrar el video en tiempo real

6. **Observa la detecci√≥n**:
   - Cajas de colores alrededor de objetos detectados
   - Etiquetas con nombre y confianza
   - FPS y n√∫mero de objetos en la esquina

7. **Panel lateral derecho muestra**:
   - **FPS**: Cuadros por segundo en tiempo real
   - **Objects**: N√∫mero de objetos en el frame actual
   - **Latency**: Tiempo de procesamiento en milisegundos
   - **Frame**: N√∫mero de frame procesado

8. **Tabla de conteo**: 
   - Muestra cu√°ntos objetos de cada clase se han detectado
   - Ejemplo: "person: 45, car: 12, dog: 3"

9. **Bot√≥n "üì∏ Snapshot"**: 
   - Guarda el frame actual en `snapshots/snapshot_[timestamp].jpg`

10. **Click en "‚èπÔ∏è Stop"** para detener

##### **Subiendo una Imagen:**

1. **Selecciona "Upload Image"** en Input Source

2. **Click en "Browse files"**

3. **Selecciona una imagen** (.jpg, .jpeg, .png)

4. **Procesamiento autom√°tico**:
   - Se detectar√°n objetos/personas/poses
   - Resultado se muestra inmediatamente
   
5. **Expandir "üîç Detection Details"** para ver:
   ```json
   {
     "frame_id": 0,
     "objects": [
       {
         "label": "person",
         "confidence": 0.95,
         "bbox": [100, 150, 300, 450]
       }
     ],
     "fps": 25.3
   }
   ```

##### **Procesando un Video:**

1. **Selecciona "Upload Video"**

2. **Sube tu video** (.mp4, .avi, .mov)

3. **Click "Process Video"**

4. **Espera el procesamiento**:
   - Ver√°s barra de progreso
   - Cada frame se procesa y anota
   
5. **Resultado**:
   - Video procesado en `results/videos/detected_[nombre].mp4`
   - Anotaciones JSON en `results/videos/detected_[nombre].json`

#### **4. Pesta√±a: üìä Metrics**

Aqu√≠ ver√°s el an√°lisis completo de rendimiento:

**M√©tricas Principales** (4 tarjetas superiores):
- **Avg FPS**: FPS promedio de toda la sesi√≥n
- **Total Frames**: Cu√°ntos frames se procesaron
- **Total Objects**: Suma de todos los objetos detectados
- **Unique Classes**: Cu√°ntas clases diferentes se detectaron

**Gr√°fica: FPS Over Time**
- L√≠nea temporal mostrando FPS en cada frame
- √ötil para identificar ca√≠das de rendimiento
- Hover sobre la l√≠nea para ver valores exactos

**Gr√°ficas: Object Class Distribution**
- **Gr√°fica de Barras**: Muestra conteo de cada clase
- **Gr√°fica de Pastel**: Proporci√≥n porcentual de cada clase
- Colores din√°micos para cada categor√≠a

#### **5. Pesta√±a: üîç Analysis**

An√°lisis detallado de las detecciones:

**Detection Timeline:**
- Gr√°fica de objetos detectados por frame
- Identifica patrones (cu√°ndo hay m√°s/menos objetos)

**Detailed Statistics (2 paneles):**

**Panel Izquierdo - FPS Statistics:**
```json
{
  "Mean": 28.45,      // FPS promedio
  "Median": 29.12,    // FPS mediano
  "Min": 18.23,       // FPS m√≠nimo (peor caso)
  "Max": 32.87,       // FPS m√°ximo (mejor caso)
  "Std Dev": 3.21     // Desviaci√≥n est√°ndar
}
```

**Panel Derecho - Detection Statistics:**
```json
{
  "Total Detections": 1847,        // Total de objetos
  "Avg per Frame": 3.2,            // Promedio por frame
  "Max per Frame": 8,              // M√°ximo en un frame
  "Frames with Objects": 573       // Frames con detecciones
}
```

#### **6. Pesta√±a: üíæ Export**

Exporta todos los datos recopilados:

**Formatos disponibles:**
- **JSON**: Estructura completa con metadatos
- **CSV**: Tabla simple para an√°lisis
- **Excel**: Formato XLSX con columnas organizadas

**Opciones:**
- ‚òëÔ∏è **Include Performance Metrics**: A√±ade FPS y tiempos
- ‚òê **Include Annotated Images**: (pr√≥ximamente)

**Proceso:**
1. Selecciona formato deseado
2. Marca opciones adicionales
3. Click "üì• Export Data"
4. Espera mensaje "‚úÖ Export completed!"
5. Click "‚¨áÔ∏è Download File" para descargar

**Ubicaci√≥n de archivos:**
- `exports/detections_[timestamp].json`
- `exports/detections_[timestamp].csv`
- `exports/detections_[timestamp].xlsx`

**Vista previa:**
- Muestra los √∫ltimos 10 frames procesados
- Tabla con Frame ID, Objects, FPS, Latency

---

### **B. Script Demo (Terminal) - Uso Detallado**

El script `demo.py` ofrece acceso directo por l√≠nea de comandos.

#### **1. Ejecuci√≥n R√°pida (Sin Argumentos)**
```bash
python demo.py
```
- Inicia directamente la **webcam con YOLO**
- Presiona **'q'** para salir
- Presiona **'s'** para guardar snapshot

#### **2. Modos Disponibles**

##### **a) YOLO con Webcam**
```bash
python demo.py yolo-webcam
```

**Con configuraci√≥n personalizada:**
```bash
python demo.py yolo-webcam --model yolov8s.pt --conf 0.6
```

**Qu√© hace:**
- Abre tu webcam predeterminada
- Muestra video con detecciones en tiempo real
- Presiona 'q' para salir
- Presiona 's' para guardar frame actual
- Al terminar, imprime m√©tricas:
  ```
  üìä Performance Metrics:
    Average FPS: 28.34
    Total Frames: 1245
    Avg Detections: 2.8
  ```

##### **b) YOLO con Imagen**
```bash
python demo.py yolo-image --input ruta/a/imagen.jpg
```

**Ejemplo completo:**
```bash
python demo.py yolo-image --input data/input/street.jpg --model yolov8m.pt --conf 0.5
```

**Qu√© hace:**
1. Carga la imagen
2. Ejecuta detecci√≥n
3. Guarda resultado anotado en `results/images/detected_street.jpg`
4. Guarda JSON en `results/images/detected_street.json`
5. Muestra imagen en ventana
6. Imprime resumen:
   ```
   üìã Detection Summary:
     Objects detected: 8
     FPS: 18.67
     Object counts:
       - person: 4
       - car: 3
       - bicycle: 1
   ```

##### **c) YOLO con Video**
```bash
python demo.py yolo-video --input ruta/a/video.mp4
```

**Ejemplo:**
```bash
python demo.py yolo-video --input data/input/traffic.mp4 --model yolov8s.pt --conf 0.5
```

**Qu√© hace:**
1. Abre el video
2. Procesa cada frame con detecci√≥n
3. Muestra progreso: "Progress: 45.2%"
4. Guarda video anotado en `results/videos/detected_traffic.mp4`
5. Guarda JSON con todas las detecciones
6. Presiona 'q' para cancelar
7. Al terminar, imprime m√©tricas completas

**Tiempo estimado:**
- Video de 1 minuto (30 FPS = 1800 frames)
- Con CPU: ~2-3 minutos
- Con GPU: ~30-45 segundos

##### **d) MediaPipe con Webcam**
```bash
python demo.py mediapipe
```

**Qu√© hace:**
- Detecta **manos** (dibuja 21 puntos por mano)
- Detecta **pose corporal** (33 puntos del cuerpo)
- Reconoce gestos b√°sicos:
  - üëä Pu√±o cerrado ‚Üí "fist"
  - ‚òùÔ∏è Un dedo ‚Üí "pointing"
  - ‚úåÔ∏è Dos dedos ‚Üí "peace"
  - üñêÔ∏è Mano abierta ‚Üí "open_palm"
- Muestra etiqueta de mano (Left/Right)
- Calcula m√©tricas de postura
- Presiona 'q' para salir

##### **e) CLIP Embeddings**
```bash
python demo.py clip --input directorio/con/imagenes/
```

**Ejemplo:**
```bash
python demo.py clip --input data/input/
```

**Qu√© hace:**
1. Carga modelo CLIP (tarda ~10 segundos la primera vez)
2. Procesa todas las im√°genes del directorio
3. Genera embeddings (vectores de 512 dimensiones)
4. Crea visualizaci√≥n PCA ‚Üí `results/embeddings/pca_visualization.png`
5. Crea visualizaci√≥n t-SNE ‚Üí `results/embeddings/tsne_visualization.png`
6. Guarda embeddings ‚Üí `results/embeddings/embeddings.npy`
7. Realiza b√∫squeda demo con queries:
   ```
   Query: 'a person walking'
     1. person_street_01.jpg     (similarity: 0.847)
     2. walking_park.jpg         (similarity: 0.783)
     3. pedestrian.jpg           (similarity: 0.721)
   ```

**Requisitos:**
- M√≠nimo 3 im√°genes en el directorio
- Formatos: .jpg, .jpeg, .png

##### **f) Modo Combinado**
```bash
python demo.py combined
```

**Qu√© hace:**
- Ejecuta YOLO + MediaPipe simult√°neamente
- Presiona:
  - **'1'** ‚Üí Solo YOLO
  - **'2'** ‚Üí Solo MediaPipe
  - **'3'** ‚Üí Ambos (modo combinado)
  - **'q'** ‚Üí Salir
- Muestra modo actual en pantalla
- √ötil para comparar rendimiento

##### **g) Ejecutar Todos los Demos**
```bash
python demo.py all --input data/input/
```

**Qu√© hace:**
1. Ejecuta demo de YOLO webcam
2. Espera a que cierres (q)
3. Ejecuta demo de MediaPipe
4. Espera a que cierres (q)
5. Ejecuta demo de CLIP (si proporcionaste --input)
6. Muestra todas las m√©tricas

#### **3. Argumentos Disponibles**

```bash
python demo.py [MODO] [OPCIONES]
```

**Modos:**
- `yolo-webcam` - YOLO con webcam
- `yolo-image` - YOLO con imagen
- `yolo-video` - YOLO con video
- `mediapipe` - MediaPipe webcam
- `clip` - Embeddings CLIP
- `combined` - YOLO + MediaPipe
- `all` - Todos los demos

**Opciones:**
- `--input PATH` - Ruta a archivo/directorio
- `--model PATH` - Modelo YOLO (default: yolov8n.pt)
- `--conf FLOAT` - Umbral de confianza (default: 0.5)

