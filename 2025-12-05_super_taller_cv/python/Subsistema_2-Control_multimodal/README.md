# Subsistema Multimodal - Voz, Gestos y EEG

## üéØ Descripci√≥n del Proyecto

Este proyecto implementa un **subsistema multimodal interactivo** que integra tres modalidades de entrada humano-computador:

- **üé§ Voz**: Reconocimiento de comandos de voz para cambiar colores y formas
- **ü§è Gestos**: Detecci√≥n de gestos manuales usando visi√≥n computacional
- **üß† EEG**: Simulaci√≥n de se√±ales electroencefalogr√°ficas con efectos visuales

El sistema permite la interacci√≥n natural con una computadora a trav√©s de m√∫ltiples canales sensoriales, demostrando las capacidades de la computaci√≥n multimodal en tiempo real.

## üë• Estudiantes del Grupo

- **Sergio Alejandro Nova P√©rez**
- **Luis Alfonso Pedraos Suarez**

## üìã ¬øQu√© Hace el Subsistema?

El subsistema multimodal proporciona una interfaz interactiva que:

1. **Escucha comandos de voz** en espa√±ol para cambiar colores (rojo, azul, verde, amarillo, morado, naranja) y formas (c√≠rculo, cuadrado, tri√°ngulo, estrella)

2. **Detecta gestos manuales** en tiempo real usando el modelo MediaPipe Hand Landmarker, espec√≠ficamente el gesto de "pinch" (pellizco) entre pulgar e √≠ndice

3. **Simula se√±ales EEG** con cambios graduales que afectan la intensidad visual de los efectos aplicados al video

4. **Aplica efectos visuales en tiempo real** basados en las entradas multimodal, incluyendo:
   - Cambio de tonalidad de color seg√∫n la voz
   - Filtro de desenfoque activable por gestos
   - Intensidad visual modulada por EEG simulado
   - Dibujo de formas geom√©tricas en el centro de la pantalla

## üîß Componentes T√©cnicos

### Hardware Requerido
- Webcam compatible con OpenCV
- Micr√≥fono para entrada de voz
- Computadora con GPU compatible (opcional, acelera procesamiento)

### Software y Librer√≠as
- **Python 3.8+**
- **OpenCV**: Procesamiento de video e im√°genes
- **MediaPipe**: Detecci√≥n de gestos y landmarks de mano
- **SpeechRecognition**: Reconocimiento de voz con Google Speech API
- **NumPy**: C√°lculos matem√°ticos y manipulaci√≥n de arrays
- **PyAudio**: Interfaz de audio para reconocimiento de voz

### Archivos del Proyecto
```
multimodal/
‚îú‚îÄ‚îÄ main.py                    # Sistema multimodal principal
‚îú‚îÄ‚îÄ requirements.txt           # Dependencias Python
‚îú‚îÄ‚îÄ hand_landmarker.task       # Modelo preentrenado MediaPipe
‚îú‚îÄ‚îÄ metrics/
‚îÇ   ‚îî‚îÄ‚îÄ multimodal_metrics.txt # M√©tricas de rendimiento
‚îú‚îÄ‚îÄ captures/                  # Capturas de pantalla (opcional)
‚îî‚îÄ‚îÄ README.md                  # Esta documentaci√≥n
```

## üéØ ¬øQu√© Problema Soluciona?

### Problema Original
Las interfaces tradicionales de computadora requieren interacci√≥n unimodal (teclado, mouse), limitando la accesibilidad y naturalidad de la interacci√≥n humano-computador.

### Soluci√≥n Implementada
El subsistema multimodal aborda esta limitaci√≥n proporcionando:

1. **Accesibilidad Mejorada**: M√∫ltiples formas de interacci√≥n para usuarios con diferentes capacidades
2. **Interacci√≥n Natural**: Comunicaci√≥n m√°s intuitiva similar a la interacci√≥n humana
3. **Experiencia Inmersiva**: Combinaci√≥n de modalidades crea una experiencia m√°s rica
4. **Demostraci√≥n Tecnol√≥gica**: Prueba de concepto de sistemas multimodal en tiempo real

## üìä M√©tricas de Rendimiento

El sistema recopila y guarda autom√°ticamente las siguientes m√©tricas en `metrics/multimodal_metrics.txt`:

### M√©tricas T√©cnicas
- **Duraci√≥n de Sesi√≥n**: Tiempo total de funcionamiento
- **Frames Procesados**: N√∫mero total de frames de video procesados
- **FPS (Frames Por Segundo)**:
  - Promedio: Rendimiento general del sistema
  - M√≠nimo: Peor caso de rendimiento
  - M√°ximo: Mejor caso de rendimiento

### M√©tricas de Interacci√≥n
- **Detecciones de Voz**: N√∫mero de comandos de voz reconocidos correctamente
- **Detecciones de Gestos**: N√∫mero de gestos detectados y procesados
- **Cambios de EEG**: N√∫mero de actualizaciones en la simulaci√≥n EEG

### Ejemplo de M√©tricas Recopiladas
```
METRICAS DEL SUBSISTEMA MULTIMODAL
========================================
Duracion sesion: 45.2 segundos
Frames procesados: 1356
FPS Promedio: 29.92
FPS Minimo: 25.10
FPS Maximo: 31.45
Detecciones de voz: 12
Detecciones de gestos: 8
Cambios de EEG: 23

MODALIDADES ACTIVAS:
- Voz: Deteccion de colores y formas
- Gestos: Pinch para activar/desactivar filtro
- EEG: Simulacion con efectos visuales
```

## üöÄ C√≥mo Usar el Sistema

### Instalaci√≥n
```bash
# Clonar o descargar el proyecto
cd multimodal

# Instalar dependencias
pip install -r requirements.txt
```

### Ejecuci√≥n
```bash
python main.py
```

### Instrucciones de Uso
1. **Voz**: Di colores ("rojo", "azul", etc.) o formas ("c√≠rculo", "cuadrado", etc.)
2. **Gestos**: Acerca pulgar e √≠ndice para activar/desactivar el filtro de desenfoque
3. **EEG**: Observa c√≥mo los cambios simulados afectan la intensidad visual
4. **Salir**: Presiona 'Q' en la ventana de video

## üìπ Demostraci√≥n en Video

[üîó Video de Demostraci√≥n en YouTube](https://youtu.be/KQVwb1arwhM)


## üîç An√°lisis T√©cnico Detallado

### Arquitectura del Sistema
- **Hilos Paralelos**: Voz se procesa en hilo separado para no bloquear video
- **Procesamiento en Tiempo Real**: 30 FPS objetivo con webcam est√°ndar
- **Modelo Preentrenado**: MediaPipe Hand Landmarker para detecci√≥n robusta de gestos
- **API de Voz**: Google Speech Recognition para reconocimiento en espa√±ol

### Algoritmos Implementados
1. **Detecci√≥n de Gestos**: C√°lculo de distancia euclidiana entre landmarks de dedos
2. **Simulaci√≥n EEG**: Ruido gaussiano controlado para cambios realistas
3. **Mezcla Visual**: Combinaci√≥n ponderada de frames originales y efectos
4. **M√©tricas de Rendimiento**: C√°lculo estad√≠stico de FPS y contadores de eventos

## üìà Conclusiones

### Logros Alcanzados
‚úÖ **Integraci√≥n Exitosa**: Tres modalidades (voz, gestos, EEG) funcionando simult√°neamente
‚úÖ **Rendimiento √ìptimo**: 30 FPS promedio en hardware est√°ndar
‚úÖ **Interfaz Intuitiva**: Interacci√≥n natural sin necesidad de entrenamiento extenso
‚úÖ **Robustez**: Sistema tolerante a errores y condiciones variables

### M√©tricas de √âxito
- **Rendimiento**: FPS promedio > 25, procesamiento en tiempo real
- **Precisi√≥n**: Alta tasa de detecci√≥n de gestos y voz
- **Usabilidad**: Interfaz simple y responsive
- **Estabilidad**: Sistema funcionando sin crashes durante sesiones prolongadas

### Limitaciones Identificadas
- Dependencia de conexi√≥n a internet para reconocimiento de voz
- Requerimiento de buena iluminaci√≥n para detecci√≥n de gestos
- Simulaci√≥n EEG (no se√±ales reales) limita aplicaciones m√©dicas

### Aplicaciones Futuras
- **Accesibilidad**: Interfaces para personas con discapacidades motoras
- **Realidad Virtual**: Controles gestuales en entornos VR/AR
- **Automoci√≥n**: Interfaces de veh√≠culo sin manos
- **Educaci√≥n**: Herramientas interactivas para aprendizaje
- **Medicina**: Interfaces para pacientes con movilidad limitada

### Recomendaciones
1. **Hardware**: Webcam HD y micr√≥fono de calidad mejoran precisi√≥n
2. **Iluminaci√≥n**: Buena luz para detecci√≥n √≥ptima de gestos
3. **Calibraci√≥n**: Ajuste de umbrales seg√∫n condiciones ambientales
4. **Extensi√≥n**: Integraci√≥n con m√°s modalidades (t√°ctil, ocular)

---

**Proyecto desarrollado como parte del curso de Sistemas Multimodales**
**Fecha de desarrollo: Diciembre 2025**</content>
<filePath">/home/brosgor/Documentos/miGit/multimodal/README.md